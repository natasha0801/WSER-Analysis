# -*- coding: utf-8 -*-
"""wser_buckle_predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ysLaz9wIrlBHfbTNm3Z5SgV7awoq4v23
"""

# Commented out IPython magic to ensure Python compatibility.
# Import packages
import warnings, logging, os
logging.disable(logging.WARNING)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
warnings.simplefilter(action='ignore', category=FutureWarning)


# %tensorflow_version 2.x
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow import feature_column as fc
import tensorflow as tf

# Useful functions to format times
def getHours(strTime):
    if strTime == "nan" or "-" in strTime:
        return 30.0
    else:
        strTime = str.split(strTime,':')
        return float(strTime[0]) + float(strTime[1])/60.0 + float(strTime[2])/3600.0

# Useful function to create input function to convert data to a tf.data.Dataset object
def make_input_fn(data_df, label_df, num_epochs=32, shuffle=True, batch_size=32):
    def input_function():
        ds = tf.data.Dataset.from_tensor_slices((dict(data_df),label_df))
        if shuffle:
            ds = ds.shuffle(1000)   # randomize order of data
        ds = ds.batch(batch_size).repeat(num_epochs)    # split into batches
        return ds
    return input_function

# Splits, features, labels
aidStationNames = ['Lyon Ridge', 'Red Star Ridge', 'Duncan Canyon', 'Robinson Flat', "Miller's Defeat", 'Dusty Corners', "Last Chance", "Devil's Thumb", "El Dorado Creek", "Michigan Bluff"]
CATEGORICAL_COLUMNS = ['Gender']
NUMERIC_COLUMNS = np.concatenate([['Age'], aidStationNames])
features = np.concatenate([CATEGORICAL_COLUMNS, NUMERIC_COLUMNS])
labels=['Time']

cutoff = 24.0   # get that silver buckle

# Load data
df_train = pd.read_csv('wser-history.csv')
df_test = pd.read_csv('wser2019.csv')

# Format times
relevantSplits = np.concatenate([aidStationNames, labels])
for i in range(0, len(relevantSplits)):
    df_train[relevantSplits[i]] = df_train[relevantSplits[i]].apply(lambda x: getHours(str(x)))
    df_test[relevantSplits[i]] = df_test[relevantSplits[i]].apply(lambda x: getHours(str(x)))

# Split into training & testing data
input_train = df_train[features].copy()
input_test = df_test[features].copy()
output_train = (df_train[labels].copy() < cutoff).astype(int)
output_test = (df_test[labels].copy() < cutoff).astype(int)

# Split into feature columns
feature_columns = []

for feature_name in CATEGORICAL_COLUMNS:
    vocabulary = input_train[feature_name].unique()
    feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name,vocabulary))

for feature_name in NUMERIC_COLUMNS:
    feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))

# Create input data
input_train_fn = make_input_fn(input_train, output_train)
input_test_fn = make_input_fn(input_test, output_test, num_epochs=1, shuffle=False)

# Create linear classifier object which creates a model for us
linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)

# Train the model!
linear_est.train(input_train_fn)

# Evaluate the model!
result = linear_est.evaluate(input_test_fn)

print(f"Accuracy: {result['accuracy']}")
